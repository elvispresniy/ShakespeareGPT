{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download \"shakespeare.txt\" dataset\n",
    "path_to_file = get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "\n",
    "idx_to_char = {i:x for i, x in enumerate(vocab)}\n",
    "char_to_idx = {x:i for i, x in enumerate(vocab)}\n",
    "\n",
    "def text_to_idx(text_line):\n",
    "  '''Converts a list of words into a list of word indices.'''\n",
    "  idxs = [char_to_idx[char] for char in text_line]\n",
    "  return idxs\n",
    "\n",
    "def idx_to_text(idx_line):\n",
    "  '''Converts a list of word indices into a list of words.'''\n",
    "  text_line = ''.join([idx_to_char[idx] for idx in idx_line])\n",
    "  return text_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_text(text_line, seq_len=129):\n",
    "    '''Slices text into feature-target pairs.'''\n",
    "    slices = [text_line[i:i + seq_len] for i in range(0, len(text_line), seq_len)]\n",
    "    return slices\n",
    "\n",
    "def pairs_to_dataloader(pairs, batch_size=32, shuffle=True):\n",
    "    '''Converts feature-target pairs into a DataLoader object.'''\n",
    "    class TextDataset(Dataset):\n",
    "        def __init__(self, pairs):\n",
    "            self.pairs = pairs\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.pairs)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            input_seq, target_seq = self.pairs[idx]\n",
    "            return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "    dataset = TextDataset(pairs)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "\n",
    "    dataloader = [(batch[0].to(device), batch[-1].to(device)) for batch in dataloader]\n",
    "    return dataloader\n",
    "\n",
    "def generate_dataloader(text, seq_len=129, batch_size=1, shuffle=True):\n",
    "    '''DIrwctly converts text into a DataLoader object with feature-target pairs.'''\n",
    "    idx_text = text_to_idx(text)\n",
    "    slices = slice_text(idx_text, seq_len=seq_len)\n",
    "\n",
    "    # Adjust slices to have pairs of input and target sequences of the same length\n",
    "    input_target_pairs = [(slice[:-1], slice[-1]) for slice in slices if len(slice) == seq_len]\n",
    "\n",
    "    dataloader = pairs_to_dataloader(input_target_pairs, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, vocab_size, dropout):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.dropout(self.embedding(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional encoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, maxlen):\n",
    "    super().__init__()\n",
    "\n",
    "    den = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model)).to(device)\n",
    "    pos = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1).to(device)\n",
    "    self.encoding = torch.zeros(maxlen, d_model).to(device)\n",
    "    self.encoding[:, 0::2] = torch.sin(pos * den)\n",
    "    self.encoding[:, 1::2] = torch.cos(pos * den)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, d_model, n_heads):\n",
    "    super().__init__()\n",
    "\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    self.query = nn.Linear(d_model, d_model)\n",
    "    self.key = nn.Linear(d_model, d_model)\n",
    "    self.value = nn.Linear(d_model, d_model)\n",
    "\n",
    "    self.scale = lambda x: x / math.sqrt(n_heads)\n",
    "\n",
    "    self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size, seq_len, emb_dim = x.size()\n",
    "\n",
    "    Q = self.query(x)\n",
    "    K = self.key(x)\n",
    "    V = self.value(x)\n",
    "\n",
    "    scores = Q @ K.transpose(-1, -2)\n",
    "    scores = self.scale(scores)\n",
    "\n",
    "    attention_weights = torch.softmax(scores, -1)\n",
    "    attended_values = attention_weights @ V\n",
    "\n",
    "    attended_values = attended_values.transpose(1, 2).contiguous()\n",
    "\n",
    "    attended_values = attended_values.view(batch_size, seq_len, emb_dim)\n",
    "\n",
    "    output = self.out(attended_values)\n",
    "\n",
    "    output += x\n",
    "\n",
    "    output = self.norm(output)\n",
    "    return output, attention_weights.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeedForwad layer for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, d_model, d_ff, dropout):\n",
    "    super().__init__()\n",
    "\n",
    "    # Linear layers\n",
    "    self.pickles = nn.Linear(d_model, d_ff)\n",
    "    self.tomatoes = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Weights normalization\n",
    "    nn.init.kaiming_normal_(self.pickles.weight, nonlinearity='relu')\n",
    "    nn.init.kaiming_normal_(self.tomatoes.weight, nonlinearity='relu')\n",
    "\n",
    "  def forward(self, x):\n",
    "    pickle = self.pickles(x)\n",
    "    pickle = F.relu(pickle)\n",
    "\n",
    "    tomato = self.tomatoes(pickle)\n",
    "    tomato = self.dropout(tomato)\n",
    "\n",
    "    output = self.norm(tomato)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder-only transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransformer(nn.Module):\n",
    "  def __init__(self, d_model, maxlen, vocab_size, dropout, n_heads, d_ff, n_att):\n",
    "    super().__init__()\n",
    "\n",
    "    # Classes\n",
    "    self.embedding = TokenEmbedding(d_model, vocab_size, dropout).to(device)\n",
    "    self.posencoding = PositionalEncoding(d_model, maxlen).to(device)\n",
    "    self.sequential_attention = [SelfAttention(d_model, n_heads).to(device) for _ in range(n_att)]\n",
    "    self.neuralnet = FeedForward(d_model, d_ff, dropout).to(device)\n",
    "\n",
    "    self.flatten = lambda x: x.view(x.size(0), -1)\n",
    "    self.out = nn.Linear(maxlen * d_model, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    embeded = self.embedding(x)\n",
    "    posencoded = self.posencoding(embeded)\n",
    "    att_Ws = []\n",
    "\n",
    "    attended = posencoded\n",
    "    for lil_attention in self.sequential_attention:\n",
    "      attended, att_W = lil_attention(attended)\n",
    "      att_Ws.append(att_W)\n",
    "\n",
    "    boring = self.neuralnet(attended)\n",
    "    flat = self.flatten(boring)\n",
    "    output = self.out(flat)\n",
    "    return output, att_Ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL     = 64\n",
    "MAXLEN      = 128\n",
    "VOCAB_SIZE  = len(vocab)\n",
    "DROPOUT     = .05\n",
    "N_HEADS     = 8\n",
    "BATCH_SIZE  = 1\n",
    "D_FF        = 1024\n",
    "N_ATT       = 2\n",
    "lr          = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=ShakespeareGPT.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShakespeareGPT = DecoderTransformer(D_MODEL, MAXLEN, VOCAB_SIZE, DROPOUT, N_HEADS, D_FF, N_ATT).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataloader\n",
    "dataloader = generate_dataloader(text, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, print_loss):\n",
    "  '''A single epoch of a training loop.'''\n",
    "  model.train()\n",
    "\n",
    "  LOSS = 0\n",
    "\n",
    "  for i, (input, target) in enumerate(dataloader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits, _ = model(input)\n",
    "\n",
    "    loss = criterion(logits, target)\n",
    "    LOSS += loss.item()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i+1)%print_loss == 0:\n",
    "      print(f'Training epoch {i+1}/{len(dataloader)}: {loss.item():.5f}')\n",
    "\n",
    "  LOSS /= len(dataloader)\n",
    "  return LOSS\n",
    "\n",
    "def get_time(epoch_time):\n",
    "  '''Converts time in seconds into minutes and seconds format.'''\n",
    "  minutes = int(epoch_time) // 60\n",
    "  seconds = epoch_time - minutes*60\n",
    "  return f'Time taken: {minutes} m. {seconds:.1f} s.'\n",
    "\n",
    "def predict_char(input, model):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    logits, _ = model(input)\n",
    "    idx = torch.argmax(logits, -1)\n",
    "  return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "print_loss = 1000\n",
    "loss_list = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "  start_time = time.time()\n",
    "  loss = train_epoch(ShakespeareGPT, print_loss)\n",
    "  loss_list.append(loss)\n",
    "  epoch_time = time.time() - start_time\n",
    "  print(f'Epoch #{epoch}: Loss = {loss:.5f}\\n{get_time(epoch_time)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions with a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token = 1000\n",
    "\n",
    "initial, target = next(iter(dataloader))\n",
    "top = initial.tolist()[0]\n",
    "input = initial\n",
    "\n",
    "for _ in range(max_token):\n",
    "  pred = predict_char(input, Optimus)\n",
    "  top.append(pred.tolist()[0])\n",
    "  input = torch.cat([input[:, 1:], pred.view(1, -1)], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'START SEQUENCE:\\n{idx_to_text(initial.tolist()[0])}', end='\\n'*3)\n",
    "print(f'PREDICTED:\\n{idx_to_text(top)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of text generated by ShakespeareGPT**\n",
    "START SEQUENCE:\n",
    "ortuned him by any means?\n",
    "\n",
    "MONTAGUE:\n",
    "Both by myself and many other friends:\n",
    "But he, his own affections' counsellor,\n",
    "Is to himsel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PREDICTED:\n",
    "ortuned him by any means?\n",
    "\n",
    "MONTAGUE:\n",
    "Both by myself and many other friends:\n",
    "But he, his own affections' counsellor,\n",
    "Is to himselfeld\n",
    " sasone sundrntwe ie tipt cerstres ee ingt\n",
    "\n",
    "Asse so se elout:\n",
    "I ho wolntee te te s yous noth yound trlind en meis nof, bole sh sorsiit ari riph, gheld go topdows:notth thmunue p mo w ch then lme ist at se sut\n",
    "I hig noull fseisthu f hal  hiy owwott bongonn yotiad your eatrln yol my toatt sourrowertay tou doin, you\n",
    "he I bea thy blay, pesctl\n",
    "They amont hir weis ghatd ton, I annedeator w yount in, tuene hs l oresain tenot\n",
    "\n",
    "bo no shot singe, I le rey lefne, sotoowise ho"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
